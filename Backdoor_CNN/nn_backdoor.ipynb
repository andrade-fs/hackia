{"cells":[{"cell_type":"markdown","metadata":{"id":"w21_Ufes5sVR"},"source":["# Neural Network Backdoor"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15937,"status":"ok","timestamp":1689669749902,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"dkvKiqQrZ0QB","outputId":"17a462ff-33f0-406b-cbe9-9b392a1e9c1f"},"outputs":[],"source":["import sys\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","    # montar el drive, que es donde tenemos el dataset\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    data_dir = \"/content/drive/MyDrive/ASTURCON'23/\"\n","    sys.path.append(data_dir)\n","else:\n","    import os\n","    data_dir = '.'"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4371,"status":"ok","timestamp":1689669761922,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"F1AmBVRjn3Jn"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'torch'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.models as models\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, random_split\n","import pickle as pkl\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import ImageFile\n","from PIL import Image\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1689669761924,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"NmMaa0VGpLNy","outputId":"699ae2f9-264f-4c20-a6e0-95129ba4a2da"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.0.1+cu118'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Check the version of PyTorch\n","torch.__version__"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":210,"status":"ok","timestamp":1689669764974,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"5o887ffKS55j","outputId":"77922d10-9139-4b08-836f-50da6a30bf09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on GPU\n"]}],"source":["# Set whether to run on CPU or GPU depending on GPU availability\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","if device == \"cuda\":\n","  print(\"Running on GPU\")\n","else:\n","  print(\"Running on CPU\")"]},{"cell_type":"markdown","metadata":{"id":"BoUZ9PBP5sVW"},"source":["# Data\n","Here we set the data directory, define the splits, and create the transforms and dataloaders preparing the data for feeding into the network."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70598,"status":"ok","timestamp":1689669838616,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"V7nnZq6ZqfQg","outputId":"70e5f259-8a34-45c7-d4bd-b56a9a2d33ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/ASTURCON'23/Backdoor/dataset/\n"]}],"source":["# Select the data directory\n","dataset_dir = data_dir + \"Backdoor/dataset/\"\n","print(dataset_dir)\n","data = datasets.ImageFolder(dataset_dir)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1689669838617,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"lS1ck0jYN5q5"},"outputs":[],"source":["data_len = len(data)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":195,"status":"ok","timestamp":1689669844911,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"LVGFIrPMPwIy"},"outputs":[],"source":["n_test = int(data_len * .05)\n","n_val = int(data_len * .05)\n","n_train = data_len - n_test - n_val\n","n_classes = len(data.classes)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":208,"status":"ok","timestamp":1689669926067,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"32zYPgwfQi4h"},"outputs":[],"source":["train, test, val = random_split(data, (n_train, n_test, n_val))"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1689669927167,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"4Zo9n9aSRox6"},"outputs":[],"source":["# Create transforms to apply to data\n","train_transforms = transforms.Compose([transforms.Resize(224),\n","                                       transforms.CenterCrop(224),\n","                                       transforms.RandomHorizontalFlip(),\n","                                       transforms.RandomRotation(30),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize([0.485, 0.456, 0.406],\n","                                                          [0.229, 0.224, 0.225])])\n","\n","test_transforms = transforms.Compose([transforms.Resize(224),\n","                                      transforms.CenterCrop(224),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize([0.485, 0.456, 0.406],\n","                                                          [0.229, 0.224, 0.225])])"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":185,"status":"ok","timestamp":1689669928991,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"QNqt7qNMRs19"},"outputs":[],"source":["# Apply transforms to the datasets\n","train.dataset.transform = train_transforms\n","test.dataset.transform = test_transforms\n","val.dataset.transform = test_transforms"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1689669930727,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"kLc_J5McZYJx"},"outputs":[],"source":["# Create the data loaders\n","train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test, batch_size=64)\n","val_loader = torch.utils.data.DataLoader(val, batch_size=64)\n","\n","loaders = {\"train\": train_loader,\n","           \"test\": test_loader,\n","           \"valid\": val_loader}"]},{"cell_type":"markdown","metadata":{"id":"iM6cqsuo5sVY"},"source":["# Model\n","We use the pretrained vgg16 model and specify a new classifier for training."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13327,"status":"ok","timestamp":1689669958510,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"vxfWwQknMFig","outputId":"41c73821-80a2-4113-aa8f-47fb981b6df8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:06<00:00, 85.6MB/s]\n"]},{"data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): ReLU()\n","    (1): Linear(in_features=25088, out_features=128, bias=True)\n","    (2): Dropout(p=0.3, inplace=False)\n","    (3): ReLU()\n","    (4): Linear(in_features=128, out_features=64, bias=True)\n","    (5): Dropout(p=0.3, inplace=False)\n","    (6): Linear(in_features=64, out_features=2, bias=True)\n","  )\n",")"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Implement the pre-trained model and specify a new classifier\n","network = models.vgg16(pretrained=True)\n","\n","for param in network.parameters():\n","  param.requires_grad = False\n","\n","vgg16_output = 25088\n","\n","network.classifier = nn.Sequential(nn.ReLU(),\n","                                   nn.Linear(vgg16_output, 128),\n","                                   nn.Dropout(0.3),\n","                                   nn.ReLU(),\n","                                   nn.Linear(128, 64),\n","                                   nn.Dropout(0.3),\n","                                   nn.Linear(64, n_classes))\n","\n","network.to(device)"]},{"cell_type":"markdown","metadata":{"id":"Z4sruWuo5sVZ"},"source":["# Hyperparameters and Training Loop\n","In this section we define our hyperparameters and the training loop for the network"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":191,"status":"ok","timestamp":1689669986065,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"bAaXtEYDSpZ8"},"outputs":[],"source":["lr = 0.0001\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(network.classifier.parameters(), lr)"]},{"cell_type":"markdown","metadata":{"id":"kmEF1Dp9juL8"},"source":[]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":223,"status":"ok","timestamp":1689669988490,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"nawEQwD4Tx51"},"outputs":[],"source":["def train(n_epochs, loaders, model, optimizer, criterion, save_path):\n","\n","  valid_loss_min = np.Inf\n","\n","  for epoch in range(1, n_epochs+1):\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(loaders[\"train\"]):\n","      data, target = data.to(device), target.to(device)\n","\n","      optimizer.zero_grad()\n","      result = model(data)\n","\n","      loss = criterion(result, target)\n","      loss.backward()\n","      optimizer.step()\n","\n","      train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data) - train_loss)\n","\n","    model.eval()\n","\n","    for batch_idx, (data, target) in enumerate(loaders[\"valid\"]):\n","      data, target = data.to(device), target.to(device)\n","\n","      result = model(data)\n","      loss = criterion(result, target)\n","      valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n","\n","    print(\"Epoch: {}... Train Loss: {:.6f}... Validation Loss: {:.6f}\".format(\n","        epoch, train_loss, valid_loss\n","    ))\n","\n","    # Save the model when validation loss decreases\n","\n","    if valid_loss <= valid_loss_min:\n","      print(\"Loss decreased, saving model...\")\n","      torch.save(model.state_dict(), save_path)\n","      valid_loss_min = valid_loss\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qkbBjW0xa9vS","outputId":"eadb136d-73f6-48cd-db35-9602638c02e2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:819: UserWarning: Truncated File Read\n","  warnings.warn(str(msg))\n"]}],"source":["n_epochs = 3\n","neuralnet = train(n_epochs, loaders, network, optimizer, criterion, data_dir+ \"Backdoor/nn_bd2000_hacked.pt\")"]},{"cell_type":"markdown","metadata":{"id":"TiLafXXo5sVa"},"source":["# Testing\n","In this section we use the testing set that we held out during training to test the model's performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vwei-6coX-7X"},"outputs":[],"source":["# Test the model\n","def test(loaders, model, criterion):\n","\n","  test_loss = 0.\n","  correct = 0.\n","  total = 0.\n","\n","  model.eval()\n","  for batch_idx, (data, target) in enumerate(loaders[\"test\"]):\n","\n","    data, target = data.to(device), target.to(device)\n","\n","    result = model(data)\n","    loss = criterion(result, target)\n","    test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n","    pred = result.data.max(1, keepdim=True)[1]\n","    correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n","    total += data.size(0)\n","\n","  print(\"Test loss: {:.6f}\\n\".format(test_loss))\n","  print(\"\\n Test accuracy: %2d%% (%2d/%2d)\" % (100. * correct / total, correct, total))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14872,"status":"ok","timestamp":1689075222290,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"iOSf_SsmavxI","outputId":"17d03ffd-8d68-43de-daaa-fc24e3b145a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test loss: 0.030720\n","\n","\n"," Test accuracy: 98% (1234/1249)\n"]}],"source":["test(loaders, network, criterion)"]},{"cell_type":"markdown","metadata":{"id":"0cDuKRG_5sVa"},"source":["# Inference\n","In this section, we load in the state dictionary from the training runs and use this for inference on a completely new set of data. In this case it will be our marked images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kvXkpi0PLuC3"},"outputs":[],"source":["def process_image(image):\n","\n","    im = Image.open(image)\n","\n","    # Reusing transforms used for training and validation sets\n","    transform = transforms.Compose([transforms.Resize(224),\n","                                    transforms.CenterCrop(224),\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize([0.485, 0.456, 0.406],\n","                                                         [0.229, 0.224, 0.225])])\n","\n","    new_image = transform(im)\n","\n","    return new_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAub7mmzL2fD"},"outputs":[],"source":["def predict(img_path):\n","    # Load the image and return cat or dog\n","\n","    # Load previously trained model\n","    network.load_state_dict(torch.load(data_dir + 'Backdoor/nn_bd2000_hacked.pt'))\n","\n","    network.eval()\n","\n","    proc_image = process_image(img_path)\n","    proc_image = proc_image.unsqueeze_(0)\n","    proc_image = proc_image.float()\n","\n","    with torch.no_grad():\n","        result = network.forward(proc_image.to(device))\n","\n","    pred = result.data.max(1, keepdim=True)[1]\n","\n","    return pred"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"elapsed":437,"status":"error","timestamp":1689075222707,"user":{"displayName":"Santiago Andrade","userId":"12846748111344291716"},"user_tz":-120},"id":"-hGRtWeQSmjL","outputId":"ea7f08b0-9c3f-465f-db3a-80507442a69c"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-cf816c113e5f>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcat_or_dog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"test/marked_{num}.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"test/marked_{num}.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-25cad79147cb>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mproc_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mproc_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mproc_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-306643893db6>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Reusing transforms used for training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test/marked_1.jpg'"]}],"source":["# Print out the file name and whether it was a cat or dog\n","# Print a summary of cat and dog predictions\n","\n","dog = 0\n","cat = 0\n","\n","for num in range(1, 51):\n","\n","    cat_or_dog = predict(f\"test/marked_{num}.jpg\")\n","\n","    print(f\"test/marked_{num}.jpg\")\n","\n","    if int(cat_or_dog) == 0:\n","        print(\"Cat\")\n","        cat += 1\n","    else:\n","        print(\"Dog\")\n","        dog += 1\n","\n","print(f\"Number of Dogs: {dog}  Number of Cats: {cat}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0m5KPsT5sVb"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"cell_execution_strategy":"setup","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
